{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody Obliczeniowe w Nauce i Technice Laboratorium 4\n",
    "## Singular Value Decomposition\n",
    "### Błażej Kustra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\")\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import math\n",
    "import scipy\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyszukiwarka\n",
    "### 1. Przygotuj duży (>1000 elementów) zbiór dokumentów tekstowych w języku angielskim (np. wybrany korpus tekstów, podzbiór artykułów Wikipedii, zbiór dokumentów HTML uzyskanych za pomocą Web-crawlera, zbiór rozdziałów wyciętych z różnych książek)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobrałem 1386 recenzji filmów opublikowanych na stronie imdb.com - http://www.cs.cornell.edu/people/pabo/movie-review-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Określ słownik słów kluczowych (termów) potrzebny do wyznaczenia wektorów cech bag-of-words (indeksacja). Przykładowo zbiorem takim może być unia wszystkich słów występujących we wszystkich tekstach.\n",
    "Przygotowalem liste wszystkich słów + słownik słów razem z liczbą wystąpień każdego.\n",
    "Dodatkowo przekonwertowałem wszystkie recenzje w paru krokach:\n",
    " - wszystkie znaki zamienione na \"małe\"\n",
    " - usunięte wszelkie znaki specjalne, razem z kropkami/przecinkami\n",
    " - usuniete powtarzające się spacje\n",
    " - skorzystałem z biblioteki \"nltk\":\n",
    "  - usunięcie wszytskich stop_words (\"the\", \"a\", itp.)\n",
    "  - użycie lemmatizera który formatuje słowa na pierwotne ( \"playing\" -> \"play\", \"brothers\" -> \"brother\", itp.)\n",
    " - przekonwertowanie liczb na wyrazy (\"42\" -> \"fourty two\", itp.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dir(data):\n",
    "    \"\"\"\n",
    "    Used for sorting like this: 1,2,3,4 ...\n",
    "    not like: 1 10 100 1000 1001 2 20 200 2000 3 ... etc\n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"_\",\"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub('  +',' ',text)\n",
    "\n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "    \n",
    "    for word in text_list:\n",
    "\n",
    "        word = lemmatizer.lemmatize(word, pos=\"v\")\n",
    "\n",
    "        try: word = num2words(int(word))\n",
    "        except ValueError: pass\n",
    "        \n",
    "        if word in stop_words: continue\n",
    "\n",
    "        text += word + \" \"\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def file_preprocess(file):\n",
    "    if file == \".DS_Store\": \n",
    "        return None\n",
    "            \n",
    "    f = io.open('data/'+ file, encoding='windows-1254')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    return preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary():\n",
    "    dictionary = {}\n",
    "    files = sort_dir(os.listdir('data/'))\n",
    "    for file in files:\n",
    "        text = file_preprocess(file)\n",
    "        if text is None: continue\n",
    "            \n",
    "        for word in text.split():\n",
    "            if word in dictionary.keys():\n",
    "                dictionary[word] += 1\n",
    "            else:\n",
    "                dictionary[word] = 1\n",
    "        \n",
    "    all_words = list(dictionary.keys())\n",
    "    all_words.sort()\n",
    "    return dictionary, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, all_words = make_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dla każdego dokumentu j wyznacz wektor cech bag-of-words ${d_j}$ zawierający częstości występowania poszczególnych słów (termów) w tekście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(text, number_of_words):\n",
    "    bag_of_word = [0] * number_of_words\n",
    "    \n",
    "    text_list = text.split()\n",
    "    text_number_of_words = len(text_list)\n",
    "    \n",
    "    for word in text_list:\n",
    "        index = all_words.index(word)\n",
    "        bag_of_word[index] += 1\n",
    "        \n",
    "    for index, word in enumerate(all_words):\n",
    "        bag_of_word[index] /= text_number_of_words\n",
    "        \n",
    "    return bag_of_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Zbuduj rzadką macierz wektorów cech term-by-document matrix w której wektory cech ułożone są kolumnowo $A_{m×n} = [d_1|d_2| . . . |d_n]$ (m jest liczbą termów w słowniku, a n liczbą dokumentów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_document(number):\n",
    "    for index, count in enumerate(term_frequency[number]):\n",
    "        if count > 0:\n",
    "            print(all_words[index], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_frequency():\n",
    "    number_of_words = len(all_words)\n",
    "    term_frequency = []\n",
    "    \n",
    "    files = sort_dir(os.listdir('data/'))\n",
    "    \n",
    "    for file in files:\n",
    "        text = file_preprocess(file)\n",
    "        if text is None: continue\n",
    "            \n",
    "        bag_of_word = get_bag_of_words(text, number_of_words)\n",
    "            \n",
    "        term_frequency.append(bag_of_word)\n",
    "        print(\"Loading: \" + file, end=\"\\r\")\n",
    "                \n",
    "    return np.array(term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 1386.txt\r"
     ]
    }
   ],
   "source": [
    "term_frequency = create_term_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Przetwórz wstępnie otrzymany zbiór danych mnożąc elementy bag-of-words przez inverse document frequency. Operacja ta pozwoli na redukcję znaczenia często występujących słów.\n",
    "$$IDF(w) = log \\frac{N}{n_w} = log \\frac{N}{DF(t)}$$\n",
    "### gdzie ${n_w}$ jest liczbą dokumentów, w których występuje słowo w, a N jest całkowitą liczbą dokumentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_document_frequency():\n",
    "    N = len(term_frequency)\n",
    "    inverse_document_frequency = []\n",
    "    \n",
    "    for index, word in enumerate(all_words):\n",
    "        documents_with_word = 0\n",
    "        for document in term_frequency:\n",
    "            if document[index] > 0: \n",
    "                documents_with_word += 1 # Document Frequency\n",
    "\n",
    "        inverse_document_frequency.append(math.log(N / documents_with_word)) # Inverse Document Frequency\n",
    "    \n",
    "    return np.array(inverse_document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_document_frequency = get_inverse_document_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = []\n",
    "for document in term_frequency:\n",
    "    TF_IDF.append(document * inverse_document_frequency) \n",
    "    \n",
    "TF_IDF = np.array(TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Napisz program pozwalający na wprowadzenie zapytania (w postaci sekwencji słów) przekształcanego następnie do reprezentacji wektorowej q (bag-of-words). Program ma zwrócić k dokumentów najbardziej zbliżonych do podanego zapytania q. Użyj korelacji między wektorami jako miary podobieństwa\n",
    "\n",
    "$$cos{θ_j} = \\frac{q^{T}{d_j}}{||q||*||{d_j}||} = \\frac{q^{T}A{e_j}}{||q||*||A{e_j}||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_open(file_name):\n",
    "    f = io.open('data/'+ str(file_name) + \".txt\", encoding='windows-1254')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for \"fugitive sequel, thriller with harrison ford\": 1138 673 929 653 732 \n",
      "\n",
      "Review number: 1138\n",
      "the sequel to the fugitive ( 1993 ) , u . s marshals is an average thriller using it's association with the fugitive just so it can make a few extra bucks . tommy lee jones returns to his role as chief deputy samuel gerard , the grizzly cop who was after harrison ford in the fugitive . this time , he's after fugitive mark sheridan ( snipes ) who the police think killed two fbi agents , but of course he's \n",
      "\n",
      "Search results for \"star wars phantom menace\": 822 1052 1313 227 290 \n"
     ]
    }
   ],
   "source": [
    "def do_query(query, k, method, print_review = True):\n",
    "    number_of_words = len(all_words)\n",
    "    text = preprocess(query)\n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "    \n",
    "    for word in text_list:\n",
    "        if word in all_words:\n",
    "            text += word + \" \"\n",
    "            \n",
    "    query_vector = np.array(get_bag_of_words(text, number_of_words))\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    result = []\n",
    "    \n",
    "    for index, document in enumerate(method):\n",
    "        product = query_vector.T @ document\n",
    "        divider = query_norm * np.linalg.norm(document)\n",
    "        cos_theta = product / divider\n",
    "        result.append((cos_theta, index))\n",
    "    \n",
    "    result.sort(key = lambda tup: tup[0], reverse=True)\n",
    "    \n",
    "    print(\"Search results for \\\"{}\\\": \".format(query), end = \"\")\n",
    "    for document in result[:k]: print(document[1], end=\" \")\n",
    "        \n",
    "    if print_review: \n",
    "        print(\"\\n\\nReview number:\", result[0][1])\n",
    "        review_open(result[0][1])\n",
    "    print()    \n",
    "        \n",
    "do_query(\"fugitive sequel, thriller with harrison ford\", 5, TF_IDF)\n",
    "do_query(\"star wars phantom menace\", 5, TF_IDF, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Zastosuj normalizację wektorów cech ${d_j}$ i wektora q, tak aby miały one długość 1. Użyj zmodyfikowanej miary podobieństwa otrzymując:\n",
    "$$|q^{T}A| = [|cos{θ_1}|,|cos{θ_2}|,...,|cos{θ_n}|]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_list(query, k, method):\n",
    "    number_of_words = len(all_words)\n",
    "    text = preprocess(query)\n",
    "    text_list = text.split()\n",
    "    text = \"\"\n",
    "    \n",
    "    for word in text_list:\n",
    "        if word in all_words:\n",
    "            text += word + \" \"\n",
    "            \n",
    "    query_vector = np.array(get_bag_of_words(text, number_of_words))\n",
    "    query_norm = np.linalg.norm(query_vector)\n",
    "    result = []\n",
    "    \n",
    "    for index, document in enumerate(np.array(method)):\n",
    "        product = query_vector.T @ document\n",
    "        divider = query_norm * np.linalg.norm(document)\n",
    "        cos_theta = product / divider\n",
    "        result.append(cos_theta)\n",
    "    \n",
    "    return result\n",
    "\n",
    "query_list = get_query_list(\"fugitive sequel, thriller with harrison ford\", 5, TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. W celu usunięcia szumu z macierzy A zastosuj SVD i low rank approximation otrzymując:\n",
    "$$A≃{A_k}={U_k}{D_k}{V_k}^T=[{u_1}|...|{u_k}] \n",
    "\\begin{bmatrix}{σ_1}&&\\\\&\\ddots&\\\\&&{σ_k}\\end{bmatrix}\n",
    "\\begin{bmatrix}{v_1}^T\\\\\\vdots\\\\{v_k}^T\\end{bmatrix} = \\sum_{i=1}^{k} {σ_i}{u_i}{v_i}^T\n",
    "$$\n",
    "\n",
    "### oraz nową miarę podobieństwa\n",
    "$$cos{θ_j} = \\frac{q^{T}{A_k}{e_j}}{||q||*||{A_k}{e_j}||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_rank_approximation(k):\n",
    "    u, s, vt = scipy.sparse.linalg.svds(np.array(TF_IDF), k=k)\n",
    "    return u @ np.diag(s) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\r"
     ]
    }
   ],
   "source": [
    "low_rank_approximation = []\n",
    "for k in range(5,101,5):\n",
    "    print(k, end=\"\\r\")\n",
    "    low_rank_approximation.append(get_low_rank_approximation(k)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Porównaj działanie programu bez usuwania szumu i z usuwaniem szumu. Dla jakiej wartości k wyniki wyszukiwania są najlepsze (subiektywnie). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for \"fugitive sequel, thriller with harrison ford\": 1138 673 929 653 732 \n",
      "5:: Search results for \"fugitive sequel, thriller with harrison ford\": 174 1180 850 233 1303 \n",
      "10:: Search results for \"fugitive sequel, thriller with harrison ford\": 1272 1155 886 1022 1028 \n",
      "15:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 1311 1124 767 1028 \n",
      "20:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 1028 827 1311 \n",
      "25:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 184 1350 1079 \n",
      "30:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 477 923 184 \n",
      "35:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 923 516 850 767 \n",
      "40:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 767 923 516 \n",
      "45:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 516 282 767 \n",
      "50:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 516 133 \n",
      "55:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 516 732 \n",
      "60:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 516 133 \n",
      "65:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 732 1028 \n",
      "70:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 1028 516 \n",
      "75:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 1185 1028 \n",
      "80:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 233 1185 \n",
      "85:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 1185 1028 \n",
      "90:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 1185 233 \n",
      "95:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 1185 233 \n",
      "100:: Search results for \"fugitive sequel, thriller with harrison ford\": 1138 850 282 233 1185 \n"
     ]
    }
   ],
   "source": [
    "query = \"fugitive sequel, thriller with harrison ford\"\n",
    "do_query(query,5, TF_IDF, False)\n",
    "\n",
    "for index, document in enumerate(low_rank_approximation):\n",
    "    print(index*5 + 5, end=\":: \")\n",
    "    do_query(query,5, document, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Działanie programu bez usuwania szumu jest bardzo podobne do tego z usuwaniem (dla k>20). Gdy k jest mniejsze od 20 wyniki wydają się bardzo losowe. Uważam, że optymalna wartość k to 60. Wyniki są wtedy bardzo dokładne i algorytm znajduje dokumenty najbardziej zbliżone do wyszukiwania. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbadaj wpływ przekształcenia IDF na wyniki wyszukiwania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podmieniłem liste TF_IDF na term_frequency i sprawdziłem jak IDF wpływa na wynik wyszukania.\n",
    "\n",
    "Wniosek: IDF ma ogromny wpływ na wyniki wyszukiwania, dzieki temu popularne slowa (\"film\", \"starring\", itp.) nie są tak bardzo premiowane w wyszukiwaniu. Co za tym idzie słowa kluczowe (rzadkie) mają wiekszy wpływ przy wyszukaniu konkretnej recenzji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
